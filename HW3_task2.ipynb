{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "In this section, we will do some feature engineering.  \n",
    "First, preprocess the data in the same way of task1.\n",
    "In this section, we try to avoid be overfitting to the training set. So, first we drop columns have less than 80% data. After that, we find that some columns have the same values such as 'Air Aspir Method' and 'Air Aspiration Method Desc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def xlsx_to_csv_pd(path):\n",
    "    data_train = pd.read_excel(path + '15.xlsx')\n",
    "    xls_16 = pd.read_excel(path + '16.xlsx')\n",
    "    xls_17 = pd.read_excel(path + '17.xlsx')\n",
    "    data_train = pd.concat([data_train,xls_16,xls_17])\n",
    "    data_test = pd.read_excel(path + '18.xlsx')\n",
    "    return data_train, data_test\n",
    "\n",
    "def x_y_split(df):\n",
    "    y = df.pop('Comb Unrd Adj FE - Conventional Fuel')\n",
    "    return df,y\n",
    "\n",
    "def choose_column(df):\n",
    "    count = df.shape[0] - df.count()\n",
    "    filtered =  count[count < len(df)*0.2]\n",
    "    index = list(filtered.index)\n",
    "    index = [col for col in index if col.find('EPA') == -1 and col.find('FE') == -1 and col.find('MPG') == -1 and col.find('CO2') == -1 and col.find('Smog') == -1 and col.find('Guzzler') == -1 and col.find('Release Date') == -1 and col.find('Mfr Name') == -1 and col.find('Verify Mfr Cd') == -1]\n",
    "    index = [col for col in index if col.find('Desc') == -1 or col.find('Calc Approach Desc') > -1 or col.find('Var Valve Timing Desc') > -1]\n",
    "    return df[index], index\n",
    "\n",
    "#load data\n",
    "train, test = xlsx_to_csv_pd('/Users/haikundu/Desktop/4995AML/hw3/')\n",
    "\n",
    "#data target split\n",
    "train_x, train_y = x_y_split(train)\n",
    "test_x, test_y = x_y_split(test)\n",
    "\n",
    "#choose column\n",
    "filtered_train, column_name = choose_column(train_x)\n",
    "filtered_test = test[column_name]\n",
    "\n",
    "#fill missing value and split data into categorical and numerical data\n",
    "def fillna_mean(dfo,index,dfto):\n",
    "    df = pd.DataFrame()\n",
    "    dft = pd.DataFrame()\n",
    "    cate = []\n",
    "    num = []\n",
    "    for col in index:\n",
    "        if dfo[col][0].dtype == 'int64' or dfo[col][0].dtype == 'float64':\n",
    "            df[col] = dfo[col].fillna(dfo.mean()[col])\n",
    "            dft[col] = dfto[col].fillna(dfo.mean()[col])\n",
    "            num.append(col)\n",
    "        else:\n",
    "            df[col] = dfo[col].fillna(dfo.mode()[col].tolist()[0]).tolist()\n",
    "            dft[col] = dfto[col].fillna(dfo.mode()[col].tolist()[0]).tolist()\n",
    "            cate.append(col)\n",
    "    return df,dft,cate,num\n",
    "\n",
    "full_train,full_test,cate,num = fillna_mean(filtered_train,column_name,filtered_test)\n",
    "cate_train = full_train[cate]\n",
    "cate_test = full_test[cate]\n",
    "num_train = full_train[num]\n",
    "num_test = full_test[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, we split the whole dataset into categorical set and numerical set.  \n",
    "In this section, we adopt two ways for feature engineering.  \n",
    "First, we applied one-hot-encoding method for categorical data. And create polynomial feature from numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(train,test):\n",
    "    full = pd.concat([train, test]) \n",
    "    extended = pd.get_dummies(full)\n",
    "    boundry = len(train)\n",
    "    train = extended[:boundry]\n",
    "    test = extended[boundry:]\n",
    "    return train, test\n",
    "\n",
    "#one hot encoding for categorical data\n",
    "train_cat,test_cat = one_hot_encoding(cate_train,cate_test)\n",
    "train_cat.index=list(range(0,len(train)))\n",
    "\n",
    "#Build non-linear features for numerical variables\n",
    "poly = PolynomialFeatures(2,include_bias= False)\n",
    "train_num = pd.DataFrame(poly.fit_transform(num_train))\n",
    "test_num = pd.DataFrame(poly.fit_transform(num_test))\n",
    "\n",
    "#combine two sub-dataset\n",
    "ohe_train = pd.concat([train_cat,train_num],axis=1)\n",
    "ohe_test = pd.concat([test_cat,test_num],axis=1)\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(ohe_train)\n",
    "X_train_scaled = scaler.transform(ohe_train)\n",
    "X_test_scaled = scaler.transform(ohe_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.976\n",
      "best parameters: {'alpha': 2.976351441631316}\n",
      "test-set score: 0.858\n"
     ]
    }
   ],
   "source": [
    "# grid search for best parameter and exam the performance of ridge regression model on test set\n",
    "param_grid_ridge = {'alpha': np.logspace(-3, 3, 20)}\n",
    "grid_ridge = GridSearchCV(Ridge(),param_grid=param_grid_ridge,cv=10,\n",
    "                          return_train_score=True)\n",
    "grid_ridge.fit(X_train_scaled, train_y)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_ridge.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_ridge.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_ridge.score(X_test_scaled, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find that the validation score for ridge regression model improved. However, the test score decreased. Based on this result, we assume that the this model may be overfitting to the training set. At the same time, we found that there are too many variables comparing to the number of training data.   \n",
    "So, we designed a second method for feature engineering.  \n",
    "In this method, first we applied count based encoding method on categorical data. Then create polynomial features from the combined whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count based encoding\n",
    "def cate_to_num(train,test,index):\n",
    "    '''\n",
    "    param:train. type: dataframe\n",
    "    param:test.  type: dataframe\n",
    "    param:col.   type: list of string\n",
    "    '''\n",
    "    boundry = len(train)\n",
    "    full = pd.concat([train, test])\n",
    "    result = pd.DataFrame()\n",
    "    for col in index:\n",
    "        result[col] = full.groupby(col)[col].transform('count')/len(full)\n",
    "    train = result[:boundry]\n",
    "    test = result[boundry:]\n",
    "    return train, test\n",
    "\n",
    "#count based encoding for categorical data\n",
    "train_count,test_count = cate_to_num(cate_train,cate_test,cate)\n",
    "\n",
    "#combine two sub-dataset\n",
    "all_train = pd.concat([train_count,num_train],axis=1)\n",
    "all_test = pd.concat([test_count,num_test],axis=1)\n",
    "\n",
    "#Build non-linear features for numerical variables\n",
    "poly_cb = PolynomialFeatures(2,include_bias= False)\n",
    "train_all = pd.DataFrame(poly_cb.fit_transform(all_train))\n",
    "test_all = pd.DataFrame(poly_cb.fit_transform(all_test))\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_all)\n",
    "X_train_scaled_cb = scaler.transform(train_all)\n",
    "X_test_scaled_cb = scaler.transform(test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.977\n",
      "best parameters: {'alpha': 2.976351441631316}\n",
      "test-set score: 0.717\n"
     ]
    }
   ],
   "source": [
    "# grid search for best parameter and exam the performance of ridge regression model on test set\n",
    "grid_ridge_cb = GridSearchCV(Ridge(),param_grid=param_grid_ridge,cv=10,\n",
    "                          return_train_score=True)\n",
    "grid_ridge_cb.fit(X_train_scaled_cb, train_y)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid_ridge_cb.best_score_))\n",
    "print(\"best parameters: {}\".format(grid_ridge_cb.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid_ridge_cb.score(X_test_scaled_cb, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we find that both ways of feature engineering are overfitting to the training set. However, for the second method, there is an increasing for the test score comparing to score in the same way of encoding categorical data without feature engineering. So, the decreasing of test socre maybe due to count-based encoding method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
